{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ],
      "metadata": {
        "id": "rT0SvqCDozZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89SwhoMJoSFU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {torch.cuda.get_device_name(torch.cuda.current_device())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are having issues connecting to a GPU you can ignore this and continue with the tutorial. The model will run on 'cpu' and will likely take longer to fit."
      ],
      "metadata": {
        "id": "y524dxz1mh13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes on Python\n",
        "\n",
        "Here is a basic refresher on python syntax. Feel free to skip this section if you are already comfortable with python.\n",
        "\n",
        "## Basic Python\n",
        "\n",
        "### Variable Types"
      ],
      "metadata": {
        "id": "FsNtXox8ei7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integer_var = 10\n",
        "float_var = 5.5\n",
        "string_var = \"Hello, World!\"\n",
        "boolean_var = True\n"
      ],
      "metadata": {
        "id": "VjK2MduaerZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional Statements"
      ],
      "metadata": {
        "id": "JrhZ5VNFetWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = 17\n",
        "\n",
        "if age < 18:\n",
        "    print(\"You are a minor.\")\n",
        "elif age >= 18 and age < 65:\n",
        "    print(\"You are an adult.\")\n",
        "else:\n",
        "    print(\"You are a senior.\")"
      ],
      "metadata": {
        "id": "9halskO9evZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loops"
      ],
      "metadata": {
        "id": "eRpPkRk_exdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for loop\n",
        "for i in range(5):  # range(5) generates numbers from 0 to 4\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "RcKkvlrCey6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# while loop\n",
        "counter = 0\n",
        "while counter < 5:\n",
        "    print(counter)\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "nGgXspure2Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "m89z-u1Oe7Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name):\n",
        "    print(\"Hello \" + name)\n",
        "\n",
        "greet(\"John\")\n",
        "greet(\"Jane\")"
      ],
      "metadata": {
        "id": "rHPJnuQ3e_EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lists\n"
      ],
      "metadata": {
        "id": "yQ3e7unde0fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fruits = [\"apple\", \"banana\", \"orange\"]\n",
        "fruits\n",
        "fruits.append(\"grape\")\n",
        "fruits\n",
        "fruits.remove(\"banana\")\n",
        "fruits\n",
        "\n",
        "fruits[0]\n",
        "fruits[0] = \"pineapple\"\n",
        "fruits"
      ],
      "metadata": {
        "id": "oGrKvO0Ve9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruits"
      ],
      "metadata": {
        "id": "0xtqmASTwn5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Libraries\n",
        "\n",
        "Python libraries are like R packages. They often have extensive documentation online. We will use the following python packages in this tutorial:\n",
        "\n",
        "- numpy: math and working with matrices\n",
        "- pandas: cleaning/wrangling data frames\n",
        "- sklearn: machine learning toolkit\n",
        "- matplotlib: plotting\n",
        "- seaborn: more plotting\n",
        "- torch: neural networks, backprop optimization"
      ],
      "metadata": {
        "id": "Tx-VpBJ0kNlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read In Data\n",
        "In this tutorial we will forecast measles incidence using neural networks in PyTorch. We will employ the seminal England and Wales pre-vaccination bi-weekly measles dataset (a description of which can be found [here](https://doi.org/10.1371/journal.pcbi.1010251)).\n",
        "We will work with the ten most populous cities from 1949-1965.  "
      ],
      "metadata": {
        "id": "3jX4xOY2YGpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# URL of the raw Parquet file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/WyattGMadden/intro_to_ml_for_id_emory/main/data/england_and_wales_measles/uk_measles_10.parquet'\n",
        "\n",
        "# Make a request to get the Parquet file and read in the dataset as a pandas dataframe\n",
        "r = requests.get(url)\n",
        "data = io.BytesIO(r.content)\n",
        "measles = pd.read_parquet(data)"
      ],
      "metadata": {
        "id": "A7Tl1KzddFwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data\n",
        "\n",
        "Lets explore the data with some plots and summaries. Note that the data includes time, city, and cases (this will be target), and standardized case lags, nearest-big-city case lags, population lags, birth lags, and nearest-big-city distances (these will be features). Lags include one-step (bi-week) lags through 130-step lags. Since we will include all lags as features, we will be predicting one-step-ahead measles using the previous 5-years of data for each time step."
      ],
      "metadata": {
        "id": "1gZfUbldmWi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(measles)"
      ],
      "metadata": {
        "id": "PHiAAd8KfDqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of method\n",
        "# first n rows of data frame\n",
        "measles.head()"
      ],
      "metadata": {
        "id": "rXdUKq27rgrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of attribute\n",
        "# list of data frame columns\n",
        "measles.columns"
      ],
      "metadata": {
        "id": "pJ650HmWriw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in measles.columns:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "wfgYG9Xv0ufB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get second column\n",
        "measles.cases"
      ],
      "metadata": {
        "id": "gJC3kHxUaz93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get value in second row, first column\n",
        "measles.iloc[1, 0]"
      ],
      "metadata": {
        "id": "a6txvqQbbHnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot all cases over time, with separate lines for each city."
      ],
      "metadata": {
        "id": "5-wwSjtlldcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=measles, x='time', y='cases', hue='city', style='city', dashes=False, alpha=0.5)\n",
        "plt.title('Measles Cases Over Time by City')\n",
        "plt.legend(title='City', bbox_to_anchor=(1.05, 1), loc=2, fontsize='x-small')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GH5CE_uSl9Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "- Make a plot of just the London cases over time."
      ],
      "metadata": {
        "id": "A4NCUI9mGa2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "london_measles = measles[measles[\"city\"] == \"London\"]\n",
        "\n",
        "# plot here:"
      ],
      "metadata": {
        "id": "6himCozcH1Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data Into Train/Test\n",
        "\n",
        "Next we split the dataset into a training and testing set. We will train on cases prior to 1960 and test on cases after and including 1960. We also standardize within-city (separate means and standard deviations for each city) and transform the target cases with f(x) = log(x + 1)\n"
      ],
      "metadata": {
        "id": "bmiZbJwuzWIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume 'measles' DataFrame is already defined with 'time', 'city', and 'cases'\n",
        "measles['cases_logplus1'] = np.log(measles['cases'] + 1)\n",
        "\n",
        "# Initialize a dictionary to store scalers for each city\n",
        "scalers = {}\n",
        "\n",
        "# Apply scaling within each city\n",
        "for city in measles['city'].unique():\n",
        "    scaler = StandardScaler()\n",
        "    mask = measles['city'] == city  # Create a mask for each city\n",
        "    measles.loc[mask, 'cases_logplus1_standardized'] = scaler.fit_transform(measles.loc[mask, ['cases_logplus1']])\n",
        "    scalers[city] = scaler  # Store the scaler for potential future use\n",
        "\n",
        "measles_train = measles[measles[\"time\"] < 60].copy()\n",
        "measles_test = measles[measles[\"time\"] >= 60].copy()\n",
        "\n",
        "# Drop non-feature columns to create feature sets\n",
        "measles_train_features = measles_train.drop([\"time\", \"city\", \"cases\", \"cases_logplus1\"], axis=1).to_numpy()\n",
        "measles_test_features = measles_test.drop([\"time\", \"city\", \"cases\", \"cases_logplus1\"], axis=1).to_numpy()\n",
        "\n",
        "# Extract targets\n",
        "measles_train_target = measles_train['cases_logplus1_standardized'].to_numpy().reshape(-1, 1)\n",
        "measles_test_target = measles_test['cases_logplus1_standardized'].to_numpy().reshape(-1, 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AjwHF8kf08qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect training feature dimensions\n",
        "measles_train_features.shape"
      ],
      "metadata": {
        "id": "VE4beU1G3bHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect test feature dimensoins\n",
        "measles_test_features.shape"
      ],
      "metadata": {
        "id": "GI_X9jMp3fhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect all targets\n",
        "measles_train_target"
      ],
      "metadata": {
        "id": "vThb_gIc8FJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Neural Net\n",
        "\n",
        "Now we are ready to train our neural network."
      ],
      "metadata": {
        "id": "gosH-umX429T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design and Run Model\n",
        "\n",
        "We use the Dataset class (included in PyTorch) to prepare the data for the model. This includes converting the numpy arrays to tensors and writing some methods (__getitem__, __len__) that are used in the fitting process.\n",
        "\n",
        "We push the Dataset instance through the Dataloader() class to create an iterable dataset that is easy to batch over in the fitting process."
      ],
      "metadata": {
        "id": "cLUveuDixZuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# data is prepared for pytorch by using the Dataset class\n",
        "# this allows the dataset to be easily batched and iterated through\n",
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "        self.len = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "# create Dataset/DataLoader class instances from measles data\n",
        "train = Data(measles_train_features, measles_train_target)\n",
        "train_loader = DataLoader(train, batch_size = 64, shuffle = True)\n",
        "\n",
        "test = Data(measles_test_features, measles_test_target)"
      ],
      "metadata": {
        "id": "k4w3p2Xq46eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the first batch of the measles training data dataloader instance\n",
        "for i, (features, targets) in enumerate(train_loader):\n",
        "  print(features.shape, targets.shape)\n",
        "  break;\n"
      ],
      "metadata": {
        "id": "R1I-EWojKPmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we design and instanciate the model architecture that we will use. We use a feed-forward neural network with one hidden layer and ReLu activation functions."
      ],
      "metadata": {
        "id": "8KyqtW63n4ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all pytorch neural nets have the same basic form (class)\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        #self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        return self.linear_relu_stack(x)\n",
        "\n",
        "\n",
        "# create the specific model instance that will be used to fit the neural net on the heart data\n",
        "model = NeuralNetwork(\n",
        "    input_dim = measles_train_features.shape[1],\n",
        "    hidden_dim = int(measles_train_features.shape[0] * (2/3)),\n",
        "    output_dim = measles_train_target.shape[1]\n",
        "    )\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "Iif2e-7vA5U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we are ready to fit the model."
      ],
      "metadata": {
        "id": "x78KxSO6oFTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# choose the optimization routine\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "# choose the number of times the model runs through all the measles data\n",
        "num_epochs = 20\n",
        "\n",
        "# create empty lists to store the train/test loss at each epoch\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "# tell pytorch that we are starting the training process\n",
        "model.train()\n",
        "\n",
        "\n",
        "# fit the model epoch * batch_size number of times.\n",
        "# Each time updates the parameters\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Starting Epoch: \" + str(epoch))\n",
        "    train_loss = 0\n",
        "    i = 0\n",
        "    for features, target in train_loader:\n",
        "\n",
        "        # keep track of batch number\n",
        "        i += 1\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # run the model on batched test features\n",
        "        pred = model(features)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        # add loss to training_loss for accounting purposes\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # backpropogate loss\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Batch number: \" + str(i))\n",
        "\n",
        "    # save train/test set performance, append to lists\n",
        "    with torch.no_grad():\n",
        "      train_loss_values.append(train_loss / i)\n",
        "      test_pred = model(test.X)\n",
        "      test_loss = loss_fn(test_pred, test.y)\n",
        "      test_loss_values.append(test_loss.item())\n",
        "\n",
        "    # print the training loss to monitor during fitting\n",
        "    print(\"Finishing Epoch: \" + str(epoch))\n",
        "    print(\"Train Loss: \" + str(train_loss_values[epoch]))\n",
        "    print(\"Test Loss: \" + str(test_loss_values[epoch]))\n",
        "\n",
        "print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "q8UM0d97D3Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model\n",
        "\n",
        "First let's plot the train/test loss over fit iterations. The test loss (orange) and train loss (blue) both decrease over epochs. While there is some leveling-off, it is likely that training the model for additional epochs would result in improved test-set performance."
      ],
      "metadata": {
        "id": "02z7yhKtxifw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.plot(range(0, len(train_loss_values)), np.log(train_loss_values), color = \"blue\")\n",
        "plt.plot(range(0, len(train_loss_values)), np.log(test_loss_values), color = \"orange\")"
      ],
      "metadata": {
        "id": "sXKwFW1YsTbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we convert the predictions to numpy arrays for ease of metrics/plotting."
      ],
      "metadata": {
        "id": "4sNdsZIVoLZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tell pytorch that we are no longer fitting the model\n",
        "# parameters will no longer update\n",
        "model.eval()\n",
        "\n",
        "# get model predictions for train/test/validation data sets\n",
        "train_preds = model(train.X)\n",
        "test_preds = model(test.X)\n",
        "\n",
        "# convert tensor of training predictions to numpy array\n",
        "train_preds_np = train_preds.detach().numpy().flatten()\n",
        "# convert tensor of training targets to numpy array\n",
        "train_np = train.y.detach().numpy().flatten()\n",
        "\n",
        "# convert tensor of test predictions to numpy array\n",
        "test_preds_np = test_preds.detach().numpy().flatten()\n",
        "# convert tensor of test targets to numpy array\n",
        "test_np = test.y.detach().numpy().flatten()\n"
      ],
      "metadata": {
        "id": "DGtZdNm2QKwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE appears low, and is slightly worse for the training set."
      ],
      "metadata": {
        "id": "PSQTcgxhoTar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mse = np.mean((train_preds_np - train_np)**2)\n",
        "test_mse = np.mean((test_preds_np - test_np)**2)\n",
        "print(\"Train MSE: \" + str(train_mse))\n",
        "print(\"Test MSE: \" + str(test_mse))"
      ],
      "metadata": {
        "id": "SAEv0XFAB1xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot (standardized) predictions and truth for all data points. Here each city is stacked on-end."
      ],
      "metadata": {
        "id": "A4hBamLVoYri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot test predictions and test target over time (all cities on-end)\n",
        "#\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(test_np, label='Actual')\n",
        "plt.plot(test_preds_np, label='Predicted')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "KKYqeGsrdNpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's unscale the data and clean up the final data set."
      ],
      "metadata": {
        "id": "GF6um-FpohRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# include predictions in original dataset\n",
        "measles_train[\"preds_logplus1_scaled\"] = train_preds_np\n",
        "measles_test[\"preds_logplus1_scaled\"] = test_preds_np\n",
        "\n",
        "# Function to unscale predictions by city\n",
        "def unscale_by_city(data):\n",
        "    for name, scaler in scalers.items():\n",
        "        if name in data['city'].unique():\n",
        "            scaled_data = data.loc[data['city'] == name, 'preds_logplus1_scaled'].values.reshape(-1, 1)\n",
        "            unscaled_data = scaler.inverse_transform(scaled_data)\n",
        "            data.loc[data['city'] == name, 'preds_logplus1_unscaled'] = unscaled_data\n",
        "\n",
        "# Unscale predictions\n",
        "unscale_by_city(measles_train)\n",
        "unscale_by_city(measles_test)\n",
        "\n",
        "# Reverse log transformation\n",
        "measles_train[\"preds\"] = np.exp(measles_train[\"preds_logplus1_unscaled\"]) - 1\n",
        "measles_test[\"preds\"] = np.exp(measles_test[\"preds_logplus1_unscaled\"]) - 1\n",
        "\n",
        "# Tag data as train or test\n",
        "measles_train[\"test_train\"] = \"train\"\n",
        "measles_test[\"test_train\"] = \"test\"\n",
        "\n",
        "# Combine back into a single dataframe\n",
        "measles_preds = pd.concat([measles_train, measles_test])"
      ],
      "metadata": {
        "id": "PdEG8NKaCo1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can assess predictions on the original scale. Let's first just plot London predictions over true case values."
      ],
      "metadata": {
        "id": "HNIUptpQondy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join origin cases on measles_preds\n",
        "measles_preds[\"true\"] = measles_preds[\"cases\"]\n",
        "\n",
        "\n",
        "#measles_train[\"true\"] = train_np\n",
        "#measles_test[\"true\"] = test_np\n",
        "\n",
        "measles_long = pd.melt(\n",
        "    measles_preds,\n",
        "    id_vars=['time', 'city', 'test_train'],\n",
        "    value_vars=['preds', 'true'],\n",
        "    var_name='case_type',\n",
        "    value_name='Cases')\n",
        "measles_london_long = measles_long[measles_long[\"city\"] == \"London\"]\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "# Use Seaborn to plot\n",
        "sns.lineplot(\n",
        "    data=measles_london_long,\n",
        "    x='time',\n",
        "    y='Cases',\n",
        "    hue='case_type',\n",
        "    style='test_train',\n",
        "    dashes=True,\n",
        "    palette='tab10'\n",
        "    )\n",
        "\n",
        "plt.title('London Predicted vs Actual Cases Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "aP4H_CZnDfMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model appears to forecase one-step-ahead fairly well, though may do better with more epochs."
      ],
      "metadata": {
        "id": "mVEYLoWpouUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "- Inspect the test/train predicted/true cases for a different city (other than London)\n",
        "- Run the model for more epochs to see if performance improves.\n",
        "- Experiment with different hyperparameters/architectures, and investigate change in performance. For example, you could change batch size, optimizer, learning rate (and other optimizer parameters), hidden layer dimension, number of hidden layers, activation function, etc..\n",
        "\n",
        "Some useful links:\n",
        "- https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "- https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "Iy-OWlYHJo_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Resources\n",
        "\n",
        "# Neural Networks/Deep Learning\n",
        "\n",
        "- guide to Google Colab: https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=-Rh3-Vt9Nev9\n",
        "\n",
        "- Fast.ai Deep Learning Course https://course19.fast.ai/index.html\n",
        "\n",
        "- MNIST in pytorch: https://colab.research.google.com/github/omarsar/pytorch_notebooks/blob/master/pytorch_quick_start.ipynb\n",
        "\n",
        "- Andrej Karpathy's (OpenAI, former director AI at Tesla) Neural Networks: Zero to Hero Youtube course https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1\n",
        "\n",
        "- UvA Deep Learning Tutorials https://uvadlc-notebooks.readthedocs.io/en/latest/index.html\n",
        "\n",
        "- Dive Into Deep Learning (applied deep learning textbook/examples) https://d2l.ai/\n",
        "\n",
        "# General Python\n",
        "\n",
        "- https://docs.python.org/3/tutorial/"
      ],
      "metadata": {
        "id": "zD8cbdvsolID"
      }
    }
  ]
}